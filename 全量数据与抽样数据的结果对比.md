<center><font size=7>**Report**</font></center>

[TOC]
# 1. 全量用户数据召回
&nbsp; &nbsp; &nbsp; &nbsp;上次的结果利用了5%用户的历史行为数据进行召回，现在不对用户进行抽样，利用所有的10万名用户的历史行为数据来召回动漫。在不考虑模型排序的情况下，下面是四种召回算法在测试集上的效果指标。为了和之前抽样的方法进行对比，对测试集做两种处理：

- 一是仍只选择先前被抽样的用户（即取全量测试集的一个子集，约2000多用户），下面用"full-sampling"指代
- 二是不作任何处理，即测试集也是全量的(大约有5万多用户)，下面用"full-full"指代；

<table>
   <tr>
      <td>K</td>
      <td>单路召回</td>
      <td>type</td>
      <td>召回率</td>
      <td>精准率</td>
      <td>命中率</td>
      <td>覆盖率(占比)</td>
      <td>覆盖率(信息熵)</td>
      <td>覆盖率(type的信息熵)</td>
   </tr>
   <tr>
      <td>10</td>
      <td>item-CF</td>
      <td>sampling</td>
      <td>2.093%</td>
      <td>10.091%</td>
      <td>41.798%</td>
      <td>10.033%</td>
      <td>6.796571003</td>
      <td>0.44078471</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>2.111%</td>
      <td>10.177%</td>
      <td>41.636%</td>
      <td>10.783%</td>
      <td>6.82783281</td>
      <td>0.43427504</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>2.112%</td>
      <td>10.205%</td>
      <td>42.644%</td>
      <td>27.594%</td>
      <td>6.871542639</td>
      <td>0.448270549</td>
   </tr>
   <tr>
      <td></td>
      <td>item-related</td>
      <td>sampling</td>
      <td>1.466%</td>
      <td>7.247%</td>
      <td>35.953%</td>
      <td>22.615%</td>
      <td>9.037735017</td>
      <td>2.061689925</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>1.466%</td>
      <td>7.247%</td>
      <td>35.953%</td>
      <td>22.615%</td>
      <td>9.037735017</td>
      <td>2.061689925</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>1.516%</td>
      <td>7.516%</td>
      <td>37.557%</td>
      <td>36.488%</td>
      <td>9.154483756</td>
      <td>2.070047713</td>
   </tr>
   <tr>
      <td></td>
      <td>gender+age</td>
      <td>sampling</td>
      <td>2.036%</td>
      <td>9.177%</td>
      <td>41.999%</td>
      <td>3.824%</td>
      <td>6.012161386</td>
      <td>1.006018038</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>1.970%</td>
      <td>8.882%</td>
      <td>41.798%</td>
      <td>3.494%</td>
      <td>5.910116474</td>
      <td>0.996346492</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>1.944%</td>
      <td>8.752%</td>
      <td>40.361%</td>
      <td>5.414%</td>
      <td>5.922009947</td>
      <td>0.997790374</td>
   </tr>
   <tr>
      <td></td>
      <td>new</td>
      <td>sampling</td>
      <td>2.445%</td>
      <td>11.000%</td>
      <td>47.561%</td>
      <td>4.709%</td>
      <td>6.582220662</td>
      <td>1.7849376</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>2.446%</td>
      <td>11.008%</td>
      <td>48.045%</td>
      <td>4.709%</td>
      <td>6.633217844</td>
      <td>1.789868269</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>2.393%</td>
      <td>10.746%</td>
      <td>47.765%</td>
      <td>6.554%</td>
      <td>6.662913969</td>
      <td>1.791633523</td>
   </tr>
   <tr>
      <td>20</td>
      <td>item-CF</td>
      <td>sampling</td>
      <td>3.709%</td>
      <td>8.972%</td>
      <td>54.252%</td>
      <td>15.102%</td>
      <td>7.555214596</td>
      <td>0.608000074</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>3.787%</td>
      <td>9.159%</td>
      <td>55.260%</td>
      <td>16.767%</td>
      <td>7.578443021</td>
      <td>0.596426918</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>3.697%</td>
      <td>8.964%</td>
      <td>54.284%</td>
      <td>37.283%</td>
      <td>7.609111999</td>
      <td>0.609215544</td>
   </tr>
   <tr>
      <td></td>
      <td>item-related</td>
      <td>sampling</td>
      <td>3.275%</td>
      <td>8.575%</td>
      <td>56.913%</td>
      <td>27.654%</td>
      <td>9.353568286</td>
      <td>2.084388883</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>3.275%</td>
      <td>8.575%</td>
      <td>56.913%</td>
      <td>27.654%</td>
      <td>9.353568286</td>
      <td>2.084388883</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>3.292%</td>
      <td>8.666%</td>
      <td>56.943%</td>
      <td>40.147%</td>
      <td>9.396956129</td>
      <td>2.089762998</td>
   </tr>
   <tr>
      <td></td>
      <td>gender+age</td>
      <td>sampling</td>
      <td>3.641%</td>
      <td>8.230%</td>
      <td>52.358%</td>
      <td>5.324%</td>
      <td>6.880284953</td>
      <td>0.951649985</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>3.603%</td>
      <td>8.150%</td>
      <td>52.116%</td>
      <td>5.084%</td>
      <td>6.812987357</td>
      <td>0.945283699</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>3.599%</td>
      <td>8.135%</td>
      <td>51.352%</td>
      <td>5.834%</td>
      <td>6.8175394</td>
      <td>0.946753244</td>
   </tr>
   <tr>
      <td></td>
      <td>new</td>
      <td>sampling</td>
      <td>4.208%</td>
      <td>9.498%</td>
      <td>57.517%</td>
      <td>5.624%</td>
      <td>6.997815551</td>
      <td>1.810619931</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-sampling</td>
      <td>4.188%</td>
      <td>9.455%</td>
      <td>57.719%</td>
      <td>5.699%</td>
      <td>7.028072573</td>
      <td>1.82174141</td>
   </tr>
   <tr>
      <td></td>
      <td></td>
      <td>full-full</td>
      <td>4.065%</td>
      <td>9.155%</td>
      <td>56.655%</td>
      <td>6.614%</td>
      <td>7.051400297</td>
      <td>1.824907309</td>
   </tr>
   <tr>
      <td></td>
   </tr>
</table>

分别比较二者和原始的"sampling"下的效果：
## 1.1 "full-sampling" v.s. "sampling"
&nbsp; &nbsp; &nbsp; &nbsp;"full-sampling"是挖掘全量用户数据，但只对抽样用户进行召回。从上表可以看出：

- "full-sampling"的人口统计召回效果相较"sampling"略有下降
- "full-sampling"的new召回和related召回效果基本不变。这是因为这两者只关心item的属性，用户行为数据的增加不太会影响这两种召回算法的计算结果（只有对新用户的new召回会受到一点影响，因为那是根据人口统计召回的结果来算的）
- "full-sampling"的item-CF召回效果，其实我觉得也没什么显著差异。。不过照理来说用户行为的增加，是很影响到物品与物品之间相似度的计算的。这个结果可能说明了我们对用户随机采样得到的物品间关系已经可以大致上代表物品间真正的关系了？

## 1.2 "full-full" v.s. "sampling"
&nbsp; &nbsp; &nbsp; &nbsp;"full-full"是挖掘全量用户数据，并且对全量测试集中的用户进行召回。从上表可以看出：

- "full-full"的人口统计召回效果以及new召回效果都有一点变差
- "full-full"的item-CF召回效果基本不变
- "full-full"的related召回效果变好了一点？但是根据related召回的算法，它只依赖于具体的动漫间关系，是定死了的，不明所以。。

# 2. 全量用户数据训练模型——增量训练方法
&nbsp; &nbsp; &nbsp; &nbsp;本来抽样后的训练集数据量大概是90万行，而全量数据是1800万行。。无法直接全量训练。查找资料后，发现xgboost模型支持增量训练的方式，其原理就是将全量训练集分batch读入，每次训练好一个模型后，下一次迭代会在上一次的模型基础上进行，保留原有的每棵树结构不变，刷新叶子节点和内部节点的统计量(分裂阈值)，也可以继续增加新的树。这里我用了增量训练的方法，对1800万行的数据集，每次读入50万行，然后逐步更新结果。树的其他参数和上次用抽样数据训练时是一样的。这里我选的树的总棵数等于100，树的最大深度设为5，学习率为0.5或是0.2(作为对比，下面只列了0.5的结果)。

&nbsp; &nbsp; &nbsp; &nbsp;下表是每一轮增量训练后，模型对测试集召回结果进行预测打分，并按照新番得分前10、旧番得分前20的规则过滤后，最终推荐结果组成的rec_list的各项指标（这里因为计算时间原因，用的测试集用户是经过抽样的，但是召回动漫是用的全量数据结果）

<table>
   <caption>Incremental training recommendation results(eta=0.5)</caption>
   <tr>
      <td>epoch</td>
      <td>precision</td>
      <td>recall</td>
      <td>hit_rate</td>
      <td>coverage</td>
      <td>entropy</td>
      <td>type_entropy</td>
      <td>auc</td>
   </tr>
   <tr>
      <td>1</td>
      <td>10.780%</td>
      <td>7.187%</td>
      <td>76.340%</td>
      <td>23.005%</td>
      <td>8.25009483</td>
      <td>1.33107236</td>
      <td>0.85127</td>
   </tr>
   <tr>
      <td>2</td>
      <td>10.863%</td>
      <td>7.243%</td>
      <td>76.703%</td>
      <td>21.761%</td>
      <td>8.179327594</td>
      <td>1.326324437</td>
      <td>0.830598</td>
   </tr>
   <tr>
      <td>3</td>
      <td>10.908%</td>
      <td>7.272%</td>
      <td>76.904%</td>
      <td>21.896%</td>
      <td>8.187790904</td>
      <td>1.335583151</td>
      <td>0.832713</td>
   </tr>
   <tr>
      <td>4</td>
      <td>10.905%</td>
      <td>7.270%</td>
      <td>77.025%</td>
      <td>21.386%</td>
      <td>8.138098667</td>
      <td>1.32198219</td>
      <td>0.831065</td>
   </tr>
   <tr>
      <td>5</td>
      <td>10.936%</td>
      <td>7.291%</td>
      <td>77.066%</td>
      <td>21.476%</td>
      <td>8.145343595</td>
      <td>1.332397805</td>
      <td>0.828201</td>
   </tr>
   <tr>
      <td>6</td>
      <td>10.928%</td>
      <td>7.286%</td>
      <td>77.388%</td>
      <td>21.791%</td>
      <td>8.131398273</td>
      <td>1.344367955</td>
      <td>0.830204</td>
   </tr>
   <tr>
      <td>7</td>
      <td>10.843%</td>
      <td>7.229%</td>
      <td>77.146%</td>
      <td>21.416%</td>
      <td>8.129478243</td>
      <td>1.349455443</td>
      <td>0.831853</td>
   </tr>
   <tr>
      <td>8</td>
      <td>10.906%</td>
      <td>7.271%</td>
      <td>77.388%</td>
      <td>20.651%</td>
      <td>8.039298036</td>
      <td>1.329400237</td>
      <td>0.833484</td>
   </tr>
   <tr>
      <td>9</td>
      <td>10.870%</td>
      <td>7.247%</td>
      <td>77.590%</td>
      <td>21.041%</td>
      <td>8.061781863</td>
      <td>1.342366539</td>
      <td>0.832128</td>
   </tr>
   <tr>
      <td>10</td>
      <td>10.994%</td>
      <td>7.329%</td>
      <td>77.630%</td>
      <td>21.056%</td>
      <td>8.059692601</td>
      <td>1.329706135</td>
      <td>0.835747</td>
   </tr>
   <tr>
      <td>11</td>
      <td>10.804%</td>
      <td>7.203%</td>
      <td>76.824%</td>
      <td>20.381%</td>
      <td>7.998965135</td>
      <td>1.338246983</td>
      <td>0.839257</td>
   </tr>
   <tr>
      <td>12</td>
      <td>10.756%</td>
      <td>7.171%</td>
      <td>76.945%</td>
      <td>20.681%</td>
      <td>8.020103911</td>
      <td>1.349093944</td>
      <td>0.837856</td>
   </tr>
   <tr>
      <td>13</td>
      <td>11.009%</td>
      <td>7.339%</td>
      <td>77.348%</td>
      <td>20.351%</td>
      <td>7.992777874</td>
      <td>1.343417575</td>
      <td>0.838917</td>
   </tr>
   <tr>
      <td>14</td>
      <td>10.979%</td>
      <td>7.320%</td>
      <td>77.267%</td>
      <td>20.501%</td>
      <td>8.005791953</td>
      <td>1.337846912</td>
      <td>0.839606</td>
   </tr>
   <tr>
      <td>15</td>
      <td>10.978%</td>
      <td>7.319%</td>
      <td>77.428%</td>
      <td>21.191%</td>
      <td>8.054802729</td>
      <td>1.349894372</td>
      <td>0.838027</td>
   </tr>
   <tr>
      <td>16</td>
      <td>10.885%</td>
      <td>7.257%</td>
      <td>77.348%</td>
      <td>20.411%</td>
      <td>8.00240963</td>
      <td>1.348462615</td>
      <td>0.841194</td>
   </tr>
   <tr>
      <td>17</td>
      <td>10.843%</td>
      <td>7.229%</td>
      <td>77.187%</td>
      <td>19.796%</td>
      <td>7.957212365</td>
      <td>1.338297092</td>
      <td>0.846647</td>
   </tr>
   <tr>
      <td>18</td>
      <td>10.863%</td>
      <td>7.243%</td>
      <td>77.308%</td>
      <td>20.561%</td>
      <td>7.979512104</td>
      <td>1.339087113</td>
      <td>0.843767</td>
   </tr>
   <tr>
      <td>19</td>
      <td>10.884%</td>
      <td>7.256%</td>
      <td>77.428%</td>
      <td>20.006%</td>
      <td>7.949794983</td>
      <td>1.330515038</td>
      <td>0.842215</td>
   </tr>
   <tr>
      <td>20</td>
      <td>10.863%</td>
      <td>7.243%</td>
      <td>77.308%</td>
      <td>20.231%</td>
      <td>7.953860286</td>
      <td>1.341617747</td>
      <td>0.849842</td>
   </tr>
   <tr>
      <td>21</td>
      <td>10.859%</td>
      <td>7.240%</td>
      <td>77.066%</td>
      <td>19.691%</td>
      <td>7.935113187</td>
      <td>1.319056763</td>
      <td>0.852094</td>
   </tr>
   <tr>
      <td>22</td>
      <td>10.851%</td>
      <td>7.234%</td>
      <td>77.267%</td>
      <td>20.081%</td>
      <td>7.937558963</td>
      <td>1.333555293</td>
      <td>0.851705</td>
   </tr>
   <tr>
      <td>23</td>
      <td>10.827%</td>
      <td>7.218%</td>
      <td>77.146%</td>
      <td>19.601%</td>
      <td>7.921928393</td>
      <td>1.328228605</td>
      <td>0.850322</td>
   </tr>
   <tr>
      <td>24</td>
      <td>10.826%</td>
      <td>7.217%</td>
      <td>77.106%</td>
      <td>19.931%</td>
      <td>7.932152175</td>
      <td>1.32700337</td>
      <td>0.847065</td>
   </tr>
   <tr>
      <td>25</td>
      <td>10.833%</td>
      <td>7.222%</td>
      <td>77.227%</td>
      <td>19.631%</td>
      <td>7.917525028</td>
      <td>1.311647425</td>
      <td>0.846931</td>
   </tr>
   <tr>
      <td>26</td>
      <td>10.767%</td>
      <td>7.178%</td>
      <td>76.703%</td>
      <td>20.096%</td>
      <td>7.943200535</td>
      <td>1.322215491</td>
      <td>0.849118</td>
   </tr>
   <tr>
      <td>27</td>
      <td>10.824%</td>
      <td>7.217%</td>
      <td>77.025%</td>
      <td>19.721%</td>
      <td>7.934107287</td>
      <td>1.331364306</td>
      <td>0.849372</td>
   </tr>
   <tr>
      <td>28</td>
      <td>10.857%</td>
      <td>7.238%</td>
      <td>76.945%</td>
      <td>19.721%</td>
      <td>7.924339066</td>
      <td>1.307033736</td>
      <td>0.847056</td>
   </tr>
   <tr>
      <td>29</td>
      <td>10.876%</td>
      <td>7.251%</td>
      <td>77.267%</td>
      <td>19.571%</td>
      <td>7.90666729</td>
      <td>1.304219135</td>
      <td>0.855284</td>
   </tr>
   <tr>
      <td>30</td>
      <td>10.849%</td>
      <td>7.233%</td>
      <td>77.066%</td>
      <td>19.676%</td>
      <td>7.91962463</td>
      <td>1.320487234</td>
      <td>0.856669</td>
   </tr>
   <tr>
      <td>31</td>
      <td>10.757%</td>
      <td>7.172%</td>
      <td>76.945%</td>
      <td>19.841%</td>
      <td>7.913022672</td>
      <td>1.331844755</td>
      <td>0.856326</td>
   </tr>
   <tr>
      <td>32</td>
      <td>10.873%</td>
      <td>7.249%</td>
      <td>77.469%</td>
      <td>19.226%</td>
      <td>7.899827374</td>
      <td>1.308632592</td>
      <td>0.853748</td>
   </tr>
   <tr>
      <td>33</td>
      <td>10.838%</td>
      <td>7.226%</td>
      <td>77.066%</td>
      <td>19.001%</td>
      <td>7.875291698</td>
      <td>1.300463287</td>
      <td>0.859726</td>
   </tr>
   <tr>
      <td>34</td>
      <td>10.815%</td>
      <td>7.210%</td>
      <td>76.945%</td>
      <td>18.926%</td>
      <td>7.877169411</td>
      <td>1.300057114</td>
      <td>0.860085</td>
   </tr>
   <tr>
      <td>35</td>
      <td>10.917%</td>
      <td>7.278%</td>
      <td>76.945%</td>
      <td>19.541%</td>
      <td>7.910592807</td>
      <td>1.304840385</td>
      <td>0.858005</td>
   </tr>
   <tr>
      <td>36</td>
      <td>10.814%</td>
      <td>7.209%</td>
      <td>76.985%</td>
      <td>19.316%</td>
      <td>7.89451112</td>
      <td>1.299674141</td>
      <td>0.863829</td>
   </tr>
   <tr>
      <td></td>
   </tr>
</table>

&nbsp; &nbsp; &nbsp; &nbsp;从上面的表格可以看出：模型的`auc`在每一轮训练后的确是呈一个逐步上升的趋势，这也说明增量训练是起到作用的。**不过其他的指标并没有一个很稳定的变化趋势**，除了命中率`hit_rate`大体上呈现一个上升的态势外，`precision`和`recall`都是不断震荡。。

![](https://user-images.githubusercontent.com/18068963/66713304-7e0e5500-eddb-11e9-9bf2-5731158d0c7e.png)


# 3. 全量数据排序后的结果
&nbsp; &nbsp; &nbsp; &nbsp;下面对两种模型参数取值（model1对应eta=0.5,model2对应eta=0.2），对比其在测试集上排序后的结果如下：

<table>
   <caption>Model sorting results</caption>
   <tr>
      <td>model</td>
      <td>type</td>
      <td>召回率</td>
      <td>精准率</td>
      <td>命中率</td>
      <td>覆盖率(占比)</td>
      <td>覆盖率(信息熵)</td>
      <td>覆盖率(type的信息熵)</td>
   </tr>
   <tr>
      <td>model 1</td>
      <td>sampling</td>
      <td>7.469%</td>
      <td>11.203%</td>
      <td>77.872%</td>
      <td>21.296%</td>
      <td>8.067632605</td>
      <td>1.333672452</td>
   </tr>
   <tr>
      <td>model 1</td>
      <td>full-sampling</td>
      <td>7.209%</td>
      <td>10.814%</td>
      <td>76.985%</td>
      <td>19.316%</td>
      <td>7.89451112</td>
      <td>1.299674141</td>
   </tr>
   <tr>
      <td>model 1</td>
      <td>full</td>
      <td>7.195%</td>
      <td>10.771%</td>
      <td>76.402%</td>
      <td>41.302%</td>
      <td>7.930573</td>
      <td>1.307486</td>
   </tr>
   <tr>
      <td>model 2</td>
      <td>sampling</td>
      <td>7.184%</td>
      <td>10.776%</td>
      <td>76.985%</td>
      <td>15.207%</td>
      <td>7.645567067</td>
      <td>1.298258445</td>
   </tr>
   <tr>
      <td>model 2</td>
      <td>full-sampling</td>
      <td>7.245%</td>
      <td>10.867%</td>
      <td>77.267%</td>
      <td>12.537%</td>
      <td>7.530033782</td>
      <td>1.246527106</td>
   </tr>
   <tr>
      <td>model 2</td>
      <td>full</td>
      <td>7.177%</td>
      <td>10.744%</td>
      <td>76.470%</td>
      <td>25.690%</td>
      <td>7.562614</td>
      <td>1.251597</td>
   </tr>
   <tr>
      <td></td>
   </tr>
</table>

&nbsp; &nbsp; &nbsp; &nbsp;从上表可以看出：

- 学习率从0.5降低到0.2后，覆盖率显著下降，其余指标变化不是很明显
- 对于"full-sampling"，即用全量数据训练，对抽样用户的召回结果进行排序，其结果和直接用抽样数据训练并预测排序相差不大
- 感觉上更低的学习率，最后的结果要稍微好一些，但是也没有什么较大的提升。**所以说用全量数据和用增量数据效果差距不大？**

# 4. Conclusion
&nbsp; &nbsp; &nbsp; &nbsp;其实我感觉用全量数据做下来的结果没啥提高，**因为我是对用户抽样，而不是纯粹随机的抽样。对用户的随机抽样可能比较好的保持了全量用户的一些共有特征吧**？请问老师接下来还有什么地方要改进的。。我感觉其实到现在也没有啥实在的东西啊。之前考虑的填补缺失打分，或者是某些列的特别使用吗？或者是别的什么？




